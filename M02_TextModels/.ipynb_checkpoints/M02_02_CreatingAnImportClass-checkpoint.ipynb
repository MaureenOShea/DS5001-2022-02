{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS 5001 Week 2 Lab: Text into Data: Class for Importing a Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class TexImporter():\n",
    "    \n",
    "    def __init__(self, src_file, OHCO, data_in='./', data_out='./', \n",
    "                src_encoding='utf-8-sig'):\n",
    "        self.src_file = src_file\n",
    "        self.data_in = data_in\n",
    "        self.data_out = data_out\n",
    "        self.src_encoding = src_encoding\n",
    "        \n",
    "    def import_source(self):\n",
    "        self.epub = open(\"{}/{}\".format(self.data_in, self.src_file), 'r', encoding=self.src_encoding)\\\n",
    "            .readlines()\n",
    "        self.lines = pd.DataFrame(self.epub, columns=['line_str'])\n",
    "        self.lines.index.name = 'line_num'\n",
    "        self.lines.line_str = self.lines.line_str.str.strip()\n",
    "        return self\n",
    "        \n",
    "    def clip_lines(self, start_line_pat, end_line_pat):\n",
    "        start = self.lines.line_str.str.match(start_line_pat)\n",
    "        end = self.lines.line_str.str.match(end_line_pat)\n",
    "        start_line_num = self.lines.loc[start].index[0]\n",
    "        end_line_num = self.lines.loc[end].index[0]\n",
    "        self.lines = self.lines.loc[start_line_num + 1 : end_line_num - 2]\n",
    "        return self\n",
    "        \n",
    "    def chunk_chapters(self, chap_title_pat):\n",
    "        chap_lines = self.lines.line_str.str.match(chap_title_pat, case=False)\n",
    "        chap_nums = [i+1 for i in range(self.lines.loc[chap_lines].shape[0])]\n",
    "        self.lines.loc[chap_lines, 'chap_num'] = chap_nums\n",
    "        self.lines.chap_num = self.lines.chap_num.ffill()\n",
    "        self.lines = self.lines.loc[~self.lines.chap_num.isna()] # Remove everything before Chapter 1\n",
    "        self.lines = self.lines.loc[~chap_lines] # Remove chapter heading lines\n",
    "        self.lines.chap_num = self.lines.chap_num.astype('int') # Convert chap_num from float to int\n",
    "        self.chaps = self.lines.groupby(OHCO[:1]).line_str.apply(lambda x: '\\n'.join(x)).to_frame() # Make big string\n",
    "        self.chaps.line_str = self.chaps.line_str.str.strip()\n",
    "        return self\n",
    "        \n",
    "    def split_paragraphs(self, para_pat=r'\\n\\n+'):\n",
    "        self.paras = self.chaps['line_str'].str.split(para_pat, expand=True).stack()\\\n",
    "            .to_frame().rename(columns={0:'para_str'})\n",
    "        self.paras.index.names = OHCO[:2]\n",
    "        self.paras['para_str'] = self.paras['para_str'].str.replace(r'\\n', ' ').str.strip()\n",
    "        self.paras = self.paras[~self.paras['para_str'].str.match(r'^\\s*$')] # Remove empty paragraphs\n",
    "        return self\n",
    "        \n",
    "    def split_sentences(self, sent_pat=r'[.?!;:]+'):\n",
    "        self.sents = self.paras['para_str'].str.split(sent_pat, expand=True).stack()\\\n",
    "            .to_frame().rename(columns={0:'sent_str'})\n",
    "        self.sents.index.names = OHCO[:3]\n",
    "        self.sents = self.sents[~self.sents['sent_str'].str.match(r'^\\s*$')] # Remove empty paragraphs\n",
    "        return self\n",
    "        \n",
    "    def split_tokens(self, token_pat=r\"[\\s',-]+\"):\n",
    "        self.tokens = self.sents['sent_str'].str.split(token_pat, expand=True).stack()\\\n",
    "            .to_frame().rename(columns={0:'token_str'})\n",
    "        self.tokens.index.names = OHCO[:4]\n",
    "        return self\n",
    "    \n",
    "    def extract_vocab(self):\n",
    "        self.tokens['term_str'] = self.tokens.token_str.replace(r'\\W+', '', regex=True)\\\n",
    "            .str.lower()\n",
    "        self.vocab = self.tokens.term_str.value_counts().to_frame('n')\\\n",
    "            .reset_index()\\\n",
    "            .rename(columns={'index':'term_str'})\n",
    "        self.vocab.index.name = 'term_id'\n",
    "        return self\n",
    "        \n",
    "    def gather(self, ohco_level=1):\n",
    "        return self.tokens.groupby(OHCO[:ohco_level]).token_str\\\n",
    "            .apply(lambda x: ' '.join(x))\\\n",
    "            .to_frame().rename(columns={'token_str':'content'})\n",
    "    \n",
    "    def export(self, prefix='foo'):\n",
    "        self.tokens.to_csv('{}/{}-TOKENS.csv'.format(prefix, self.data_out))\n",
    "        self.vocab.to_csv('{}/{}-VOCAB.csv'.format(prefix, self.data_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in = './data_in'\n",
    "data_out = './data_out'\n",
    "epub_file = \"pg105.txt\" # Source file\n",
    "csv_file = 'austen-persuasion.csv' # The file we will create\n",
    "OHCO = ['chap_num', 'para_num', 'sent_num', 'token_num'] \n",
    "start_line_pat = r\"\\*\\*\\*\\s*START OF (THE|THIS) PROJECT\"\n",
    "end_line_pat = r\"\\*\\*\\*\\s*END OF (THE|THIS) PROJECT\"\n",
    "chap_title_pat = r\"^\\s*(chapter|letter)\\s+(\\d+)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti = TexImporter(epub_file, OHCO, data_in=data_in, data_out=data_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti.import_source();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti.clip_lines(start_line_pat, end_line_pat);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ti.lines.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti.chunk_chapters(chap_title_pat);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ti.chaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti.split_paragraphs();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ti.paras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti.split_sentences();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ti.sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ti.split_tokens();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ti.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti.extract_vocab();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ti.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti2 = TexImporter(epub_file, OHCO, data_in=data_in, data_out=data_out)\\\n",
    "    .import_source()\\\n",
    "    .clip_lines(start_line_pat, end_line_pat)\\\n",
    "    .chunk_chapters(chap_title_pat)\\\n",
    "    .split_paragraphs()\\\n",
    "    .split_sentences()\\\n",
    "    .split_tokens()\\\n",
    "    .extract_vocab()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
