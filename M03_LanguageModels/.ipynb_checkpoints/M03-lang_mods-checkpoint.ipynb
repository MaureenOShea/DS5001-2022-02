{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ai-gvPnADykO",
    "toc-hr-collapsed": true
   },
   "source": [
    "# DS 5001 Week 3 Lab: Inferring Language Models\n",
    "\n",
    "We now create a series of langage models and evaluate them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-5x8B8RODykY",
    "toc-hr-collapsed": true
   },
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fb3ZsuIsDykn"
   },
   "outputs": [],
   "source": [
    "OHCO = ['book_id', 'chap_num', 'para_num', 'sent_num', 'token_num']\n",
    "text_file1 = 'austen-persuasion.csv'\n",
    "text_file2 = 'austen-sense.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MwrVU8kZDykb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and combine texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = pd.read_csv(text_file1)\n",
    "text2 = pd.read_csv(text_file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1['book_id'] = 1\n",
    "text2['book_id'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = pd.concat([text1, text2]).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokens.set_index(OHCO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens['term_str'] = tokens['token_str'].str.lower().str.replace(r'[\\W_]', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokens['term_str'].value_counts()\\\n",
    "    .to_frame()\\\n",
    "    .reset_index()\\\n",
    "    .rename(columns={'term_str':'n', 'index':'term_str'})\\\n",
    "    .sort_values('term_str')\n",
    "vocab.index.name = 'term_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Unigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = vocab.n.sum()\n",
    "vocab['p'] = vocab['n'] / n_tokens\n",
    "vocab['log_p'] = np.log2(vocab['p'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.sort_values('p', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "NfMtOiCYDylX",
    "toc-hr-collapsed": false
   },
   "outputs": [],
   "source": [
    "smooth = vocab['p'].min()\n",
    "def predict_sentence(sent_str):\n",
    "    \n",
    "    # Parse sentence into tokens and normalize string\n",
    "    tokens = pd.DataFrame(sent_str.lower().split(), columns=['term_str'])\n",
    "    \n",
    "    # Link the tokens with model vocabulary\n",
    "    tokens = tokens.merge(vocab, on='term_str', how='left') # Left join is key\n",
    "    \n",
    "    # Add minimum values where token is not in our vocabulary\n",
    "    tokens.loc[tokens['p'].isna(), 'p'] = [smooth]\n",
    "    \n",
    "    # Compute probability of sentence by getting product of token probabilities\n",
    "    p = tokens['p'].product()\n",
    "        \n",
    "    # Print results\n",
    "    print(\"p('{}') = {}\".format(sent_str, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "NfMtOiCYDylX",
    "toc-hr-collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_sentence('I love you')\n",
    "predict_sentence('I love cars')\n",
    "predict_sentence(\"I want to\")\n",
    "predict_sentence(\"anne said to\")\n",
    "predict_sentence(\"said to her\")\n",
    "predict_sentence('said to him')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NfMtOiCYDylX",
    "toc-hr-collapsed": true
   },
   "source": [
    "## N-Gram models\n",
    "\n",
    "This function generates models up to the length specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cPUTfjSDDylz"
   },
   "outputs": [],
   "source": [
    "def get_ngrams(tokens, n=2):\n",
    "    \n",
    "    global OHCO\n",
    "    \n",
    "    # Create list to store copies of tokens table\n",
    "    X = []\n",
    "    \n",
    "    # Convert the index to cols in order to change the value of token_num\n",
    "    X.append(tokens['term_str'].reset_index())\n",
    "        \n",
    "    # Create copies of token table for each level of ngram, offset by 1, and \n",
    "    # merge with previous \n",
    "    for i in range(1, n):\n",
    "        X.append(X[0].copy())\n",
    "        X[i]['token_num'] = X[i]['token_num'] + i\n",
    "        X[i] = X[i].merge(X[i-1], on=OHCO, how='left', sort=True).fillna('<s>')\n",
    "        \n",
    "    # Compress tables to unique ngrams with counts\n",
    "    for i in range(0, n):\n",
    "        X[i] = X[i].drop(OHCO, 1)\n",
    "        cols = X[i].columns.tolist()\n",
    "        X[i]['n'] = 0\n",
    "        X[i] = X[i].groupby(cols).n.apply(lambda x: x.count()).to_frame()\n",
    "        X[i].index.names = ['w{}'.format(j) for j in range(i+1)]\n",
    "            \n",
    "    # Return just the ngram tables\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_aQs_gk_Dyl7"
   },
   "source": [
    "### Generate three models\n",
    "\n",
    "Unigram, bigram, and trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_aQs_gk_Dyl7"
   },
   "outputs": [],
   "source": [
    "m1, m2, m3 = get_ngrams(tokens, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_aQs_gk_Dyl7"
   },
   "outputs": [],
   "source": [
    "# m3.sort_values('n', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute joint probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_aQs_gk_Dyl7"
   },
   "outputs": [],
   "source": [
    "m1['p'] = m1['n'] / m1['n'].sum()\n",
    "m2['p'] = m2['n'] / m2['n'].sum()\n",
    "m3['p'] = m3['n'] / m3['n'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_aQs_gk_Dyl7"
   },
   "outputs": [],
   "source": [
    "m1.sort_values('p', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_aQs_gk_Dyl7"
   },
   "outputs": [],
   "source": [
    "m2.sort_values('p', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3.sort_values('p', ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute conditional probabilities\n",
    "\n",
    "$p(w_1|w_0) = p(w_0, w_1) / p(w_0)$\n",
    "\n",
    "$p(w_2|w_0,w_1) = p(w_0, w_1, w_2) / p(w_0, w_1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2m = m2.n.unstack().fillna(0).apply(lambda x: x / x.sum(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3m = m3.n.unstack().fillna(0).apply(lambda x: x / x.sum(), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentence2(sent_str, n=2):\n",
    "    \n",
    "    # Pick appropriate model\n",
    "    global m1, m2, m3\n",
    "    if n == 1:\n",
    "        M = m1\n",
    "    elif n == 2:\n",
    "        M = m2\n",
    "    elif n == 3:\n",
    "        M = m3\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "    # Get smoothing \n",
    "    smooth = M.p.min()\n",
    "    \n",
    "    # Add sentence padding (Hacky)\n",
    "    padded_sent_str = sent_str + (' <s>' * (n-1))\n",
    "    \n",
    "    # Parse sentence into tokens and normalize string\n",
    "    tokens = pd.DataFrame(padded_sent_str.lower().split(), columns=['term_str'])\n",
    "    \n",
    "    # Generate ngram keys \n",
    "    ngrams = []\n",
    "    offset = n - 1\n",
    "    for i in range(offset, tokens.shape[0]):\n",
    "        ngram = []\n",
    "        w = tokens.iloc[i].term_str\n",
    "        for j in range(n):\n",
    "            ngram.append(tokens.iloc[i-j].term_str)\n",
    "        ngram.reverse()\n",
    "        ngrams.append(ngram)\n",
    "        \n",
    "    # Compute the probability of the sentence\n",
    "    L = 0\n",
    "    for ngram in ngrams:\n",
    "        try:\n",
    "            p_ngram = M.loc[tuple(ngram)].p\n",
    "        except KeyError:\n",
    "            p_ngram = smooth\n",
    "        L += np.log2(p_ngram)\n",
    "    P = np.exp(L)\n",
    "    \n",
    "    print(sent_str, P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sentence2('I love you', 1)\n",
    "predict_sentence2('I love cars', 1)\n",
    "predict_sentence2(\"I want to\", 1)\n",
    "predict_sentence2(\"anne said to\", 1)\n",
    "predict_sentence2(\"said to her\", 1)\n",
    "predict_sentence2('said to him', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sentence2('I love you', 2)\n",
    "predict_sentence2('I love cars', 2)\n",
    "predict_sentence2(\"I want to\", 2)\n",
    "predict_sentence2(\"anne said to\", 2)\n",
    "predict_sentence2(\"said to her\", 2)\n",
    "predict_sentence2('said to him', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sentence2('I love you', 3)\n",
    "predict_sentence2('I love cars', 3)\n",
    "predict_sentence2(\"I want to\", 2)\n",
    "predict_sentence2(\"anne said to\", 3)\n",
    "predict_sentence2(\"said to her\", 3)\n",
    "predict_sentence2('said to him', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2m.loc[['he','she','it','anne','wentworth'], \n",
    "        ['is','had','was','felt','thought','looked','said','saw']]\\\n",
    "    .style.background_gradient(cmap='Greens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2m.loc[['he','she'],['felt','said']].style.background_gradient(cmap='Greens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cPY7ekfXgbE_"
   },
   "source": [
    "## Generate Text\n",
    "\n",
    "We use back-off to account for missing ngrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(start_word='she', n=250):\n",
    "    words = [start_word]\n",
    "    for i in range(n):\n",
    "        if len(words) == 1:\n",
    "            w = m2m.loc[start_word]\n",
    "            next_word = m2m.loc[start_word].sample(weights=w).index.values[0]\n",
    "        elif len(words) > 1:\n",
    "            bg = tuple(words[-2:])\n",
    "            try:\n",
    "                w = m3m.loc[bg]\n",
    "                next_word = m3m.loc[bg].sample(weights=w).index.values[0]\n",
    "            except KeyError:\n",
    "                ug = bg[1]\n",
    "                if ug == '<s>':\n",
    "                    next_word = m1.sample(weights=m1.p).index[0]\n",
    "                else:\n",
    "                    w = m2m.loc[ug]\n",
    "                    next_word = m2m.loc[ug].sample(weights=w).index.values[0]\n",
    "        words.append(next_word)\n",
    "    text = ' '.join(words)\n",
    "    text = text.replace(' <s> <s>', '.') + '.'\n",
    "    text = text.upper() # To give that telegraph message look :-)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text('she')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DS5559_LMs.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
